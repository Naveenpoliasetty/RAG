{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "from doman_dict import full_domain_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f4e2f",
   "metadata": {},
   "source": [
    "## Parse the resumes into directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1020190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('heoolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a188193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resumes.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "parsed_data = {}\n",
    "\n",
    "def extract_resume_category(url: str) -> str:\n",
    "    match = re.search(r\"/resume-database/\\d+-([a-zA-Z0-9-]+)/\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "for key, value in data.items():\n",
    "    role = extract_resume_category(key)  # e.g., \"oracle-resumes\"\n",
    "\n",
    "    # Initialize list if role not in parsed_data\n",
    "    if role not in parsed_data:\n",
    "        parsed_data[role] = []\n",
    "\n",
    "    # Append the current resume to the list\n",
    "    parsed_data[role].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21175b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('resumes', exist_ok=True)\n",
    "for role in parsed_data.keys():\n",
    "    with open(f'resumes/{role}.json', 'w') as f:\n",
    "        resume = json.dumps(parsed_data[role], indent=4)\n",
    "        f.write(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f537e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3f6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10468aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4991851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_match(input_string: str, data_dict: dict):\n",
    "    input_string = input_string.lower()\n",
    "    best_score = 0\n",
    "    best_key = None\n",
    "    best_value = None\n",
    "\n",
    "    for key, values in data_dict.items():\n",
    "        if not isinstance(values, list):\n",
    "            values = [values]\n",
    "        \n",
    "        # Compare input_string against all values in the list using char-level similarity\n",
    "        match, score, _ = process.extractOne(input_string, values, scorer=fuzz.ratio)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_key = key\n",
    "            best_value = match\n",
    "\n",
    "    return best_key, best_value, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c906b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_key, best_value, _ = lexical_match(\"ORACLE DeveLoper\", full_domain_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_key, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parsed_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data['business-analyst-resumes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(parsed_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"resume_embeddings\",\n",
    "    embeddings=embedding_function\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"AI Engineer at Databricks\",\n",
    "    \"Machine Learning Intern at Tesla\",\n",
    "    \"Data Scientist at OpenAI\"\n",
    "]\n",
    "\n",
    "vectorstore.add_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_section(section_name, collection_name):\n",
    "    \"\"\"Ingests a specific section (e.g., summary, skills) into Qdrant\"\"\"\n",
    "    print(f\"\\nüöÄ Ingesting {section_name.upper()} into collection: {collection_name}\")\n",
    "\n",
    "    texts, metadatas, ids = [], [], []\n",
    "\n",
    "    for idx, resume in enumerate(tqdm(resumes, desc=f\"Processing {section_name}\")):\n",
    "        # Fix: handle case where each resume is a string\n",
    "        if isinstance(resume, str):\n",
    "            resume = json.loads(resume)\n",
    "\n",
    "        text = resume.get(section_name, \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        texts.append(text)\n",
    "        metadatas.append({\n",
    "            \"resume_id\": idx,\n",
    "            \"section\": section_name\n",
    "        })\n",
    "        ids.append(f\"{section_name}_{idx}\")\n",
    "\n",
    "    if len(texts) == 0:\n",
    "        print(f\"‚ö†Ô∏è No data found for section: {section_name}\")\n",
    "        return\n",
    "\n",
    "    # Create collection in Qdrant\n",
    "    Qdrant.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "        url=QDRANT_URL,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    print(f\"‚úÖ {section_name} section ingested into Qdrant collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e096d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = RecursiveCharacterTextSplitter(chunk_size=512,chunk_overlap=50)\n",
    "chunks = text_split.split_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
