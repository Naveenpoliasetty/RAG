{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e78d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/happyuser/vscode/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-20 19:52:27 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28645a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://localhost:8000/v1/completions\"\n",
    "\n",
    "prompts = [\n",
    "    \"Explain quantum entanglement in one short paragraph.\",\n",
    "    \"Write a haiku about a GPU working late at night.\"\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"prompt\": prompts,\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "    \"stream\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2655e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.18866359599997\n",
      "\n",
      "=== Batch Inference Results ===\n",
      "\n",
      "[Prompt 1]\n",
      "Quantum entanglement is a phenomenon where two or more particles become correlated in such a way that the state of one particle cannot be described independently of the others, even when they are separated by large distances. This means that measuring the state of one particle will instantaneously affect the state of the other entangled particles, regardless of the distance between them. This has been experimentally confirmed in various systems, including photons, electrons, and even large-scale objects like superconducting circuits. The phenomenon of quantum entanglement has been observed in many different experiments, including the famous Einstein-Podolsky-Rosen (EPR) paradox, which was designed to test the completeness of quantum mechanics. Quantum entanglement has many practical applications, including quantum computing, cryptography, and quantum teleportation. Quantum entanglement is a fundamental aspect of quantum mechanics, and it has been experimentally confirmed in many different systems. It is a key feature of quantum mechanics that has been extensively studied and has many potential applications\n",
      "\n",
      "[Prompt 2]\n",
      "Silent, darkened room\n",
      "GPU's gentle hum fills\n",
      "Golden calculations\n",
      "In this haiku, I've tried to capture the quiet, nocturnal atmosphere of a computer room, where the GPU is the sole source of noise. The \"golden calculations\" line highlights the GPU's ability to perform complex computations, and the phrase \"gentle hum\" conveys a sense of soothing, steady work. The haiku's structure and syllable count are traditional, with a 5-7-5 pattern.Â \n",
      "Here's a breakdown of the haiku's imagery and themes:\n",
      "* The \"silent, darkened room\" creates a sense of stillness and quiet, emphasizing the GPU's work as the only source of activity.\n",
      "* The \"GPU's gentle hum fills\" line introduces the sound of the GPU's operation, which is often described as a gentle, soothing noise.\n",
      "* The \"golden calculations\" line highlights the GPU's ability to perform complex, high-performance computations, often used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "response = requests.post(API_URL, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
    "elapsed = time.perf_counter() - start\n",
    "print(elapsed)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    print(\"\\n=== Batch Inference Results ===\\n\")\n",
    "    for i, choice in enumerate(results[\"choices\"]):\n",
    "        print(f\"[Prompt {i+1}]\\n{choice['text'].strip()}\\n\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15f576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inference time: 10.19s | Tokens: 400 | TPS: 39.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    usage = response.json()[\"usage\"]\n",
    "    total_tokens = usage[\"completion_tokens\"]\n",
    "    tps = total_tokens / elapsed\n",
    "    print(f\"\\n Inference time: {elapsed:.2f}s | Tokens: {total_tokens} | TPS: {tps:.2f}\\n\")\n",
    "except KeyError:\n",
    "    print(f\"\\n Inference time: {elapsed:.2f}s (usage stats not available)\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
