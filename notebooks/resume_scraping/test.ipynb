{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb64add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import time\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import html2text\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674d3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_resume_markdown(url: str, session=None) -> str:\n",
    "    \"\"\"Fetch a resume page and convert main content to Markdown-like text.\"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # find main content blocks\n",
    "    target_divs = soup.find_all(\"div\", class_=[\"media-body\", \"single-post-body\"])\n",
    "    if not target_divs:\n",
    "        return \"\"\n",
    "\n",
    "    markdown_parts = []\n",
    "\n",
    "    for div in target_divs:\n",
    "        div_html = str(div)\n",
    "        tree = html.fromstring(div_html)\n",
    "\n",
    "        for element in tree.iter():\n",
    "            tag = element.tag.lower()\n",
    "\n",
    "            # headings\n",
    "            if tag in [\"h1\", \"h2\", \"h3\", \"u\", \"strong\", \"b\"]:\n",
    "                text = element.text_content().strip()\n",
    "                if text:\n",
    "                    markdown_parts.append(f\"\\n# {text}\\n\")\n",
    "\n",
    "            # paragraphs\n",
    "            elif tag == \"p\":\n",
    "                text = element.text_content().strip()\n",
    "                if text:\n",
    "                    markdown_parts.append(f\"{text}\\n\")\n",
    "\n",
    "            # unordered lists\n",
    "            elif tag == \"ul\":\n",
    "                for li in element:\n",
    "                    li_text = li.text_content().strip()\n",
    "                    if li_text:\n",
    "                        markdown_parts.append(f\"- {li_text}\\n\")\n",
    "\n",
    "            # ordered lists\n",
    "            elif tag == \"ol\":\n",
    "                for i, li in enumerate(element, 1):\n",
    "                    li_text = li.text_content().strip()\n",
    "                    if li_text:\n",
    "                        markdown_parts.append(f\"{i}. {li_text}\\n\")\n",
    "\n",
    "    markdown_text = \"\".join(markdown_parts)\n",
    "    return markdown_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741a708c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m markdown_resume = scrape_resume_markdown(url=\u001b[43mtest_urls\u001b[49m[\u001b[32m4\u001b[39m])\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(markdown_resume)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_urls' is not defined"
     ]
    }
   ],
   "source": [
    "markdown_resume = scrape_resume_markdown(url=test_urls[4])\n",
    "print(markdown_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42469491",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [\n",
    "    \"https://www.hireitpeople.com/resume-database/80-peoplesoft-resumes/627367-peoplesoft-hcm-hrms-functional-consultant-resume-3\",\n",
    "    \"https://www.hireitpeople.com/resume-database/80-peoplesoft-resumes/613216-peoplesoft-functional-consultant-business-analyst-resume-atlanta-ga-4\",\n",
    "    \"https://www.hireitpeople.com/resume-database/69-help-desk-support-resumes/614271-help-desk-support-trading-floor-support-resume-new-york-ny\",\n",
    "    \"https://www.hireitpeople.com/resume-database/69-help-desk-support-resumes/606776-computer-technician-field-associate-resume-new-york-ny-11\",\n",
    "    \"https://www.hireitpeople.com/resume-database/63-net-developers-architects-resumes/442-net-developeranalystarchitect-resume-\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_resume(url: str, session=None) -> dict:\n",
    "    \"\"\"Scrape details from an individual resume page and return plain text.\"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    data = {}\n",
    "\n",
    "    target_divs = soup.find_all(\"div\", class_=[\"single-post-body\"])\n",
    "    print(target_divs)\n",
    "    text_blocks = []\n",
    "    for div in target_divs:\n",
    "        text = div.get_text(separator=\" \", strip=True)\n",
    "        if text:\n",
    "            text_blocks.append(text)\n",
    "\n",
    "    clean_text = \"\\n\\n\".join(text_blocks)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f54e934",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = scrape_resume(url=\u001b[43mtest_urls\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(data)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_urls' is not defined"
     ]
    }
   ],
   "source": [
    "data = scrape_resume(url=test_urls[0])\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list(ul):\n",
    "    if isinstance(ul, str):\n",
    "        soup = BeautifulSoup(ul, \"lxml\")\n",
    "        ul_tag = soup.find(\"ul\")\n",
    "        if not ul_tag:\n",
    "            return []\n",
    "    elif isinstance(ul, Tag):\n",
    "        ul_tag = ul\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    points = []\n",
    "    for li in ul_tag.find_all(\"li\", recursive=False):\n",
    "        text = li.get_text(\" \", strip=True)\n",
    "        if text:\n",
    "            points.append(text)\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6ac827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"media-body\">\n",
      "<div>\n",
      "<h3>Teradata Dba Resume </h3>\n",
      "<div class=\"resume-rated mt-0\">\n",
      "<div class=\"rating-group\">\n",
      "<input checked=\"\" class=\"rating__input rating__input--none\" disabled=\"\" id=\"rated-none\" readonly=\"\" type=\"radio\" value=\"0\"/>\n",
      "<label aria-label=\"1 star\" class=\"rating__label\" for=\"rated-1\"><i class=\"rating__icon rating__icon--star fa fa-star\"></i></label>\n",
      "<input class=\"rating__input\" disabled=\"\" id=\"rated-1\" readonly=\"\" type=\"radio\" value=\"1\"/>\n",
      "<label aria-label=\"2 stars\" class=\"rating__label\" for=\"rated-2\"><i class=\"rating__icon rating__icon--star fa fa-star\"></i></label>\n",
      "<input class=\"rating__input\" disabled=\"\" id=\"rated-2\" readonly=\"\" type=\"radio\" value=\"2\"/>\n",
      "<label aria-label=\"3 stars\" class=\"rating__label\" for=\"rated-3\"><i class=\"rating__icon rating__icon--star fa fa-star\"></i></label>\n",
      "<input class=\"rating__input\" disabled=\"\" id=\"rated-3\" readonly=\"\" type=\"radio\" value=\"3\"/>\n",
      "<label aria-label=\"4 stars\" class=\"rating__label\" for=\"rated-4\"><i class=\"rating__icon rating__icon--star fa fa-star\"></i></label>\n",
      "<input class=\"rating__input\" disabled=\"\" id=\"rated-4\" readonly=\"\" type=\"radio\" value=\"4\"/>\n",
      "<label aria-label=\"5 stars\" class=\"rating__label\" for=\"rated-5\"><i class=\"rating__icon rating__icon--star fa fa-star\"></i></label>\n",
      "<input class=\"rating__input\" disabled=\"\" id=\"rated-5\" readonly=\"\" type=\"radio\" value=\"5\"/>\n",
      "</div>\n",
      "<span><b>0</b><small>/5</small> (<span class=\"submit-feedback-link\" id=\"feedback\">Submit Your Rating</span>)</span>\n",
      "</div>\n",
      "</div>\n",
      "<h4 class=\"text-greish\"><i class=\"fa fa-map-marker\"></i>Hoffman Estates, IL                                </h4>\n",
      "</div>, <div class=\"single-post-body\">\n",
      "<p>\n",
      "</p><p><u><strong>OBJECTIVE</strong></u></p>\n",
      "<ul>\n",
      "<li>Position where Database Designing and DBA skills and experience in database designing, administration and Management will add value in todayâ€™s challenging business and Information Technology.</li>\n",
      "</ul>\n",
      "<p><u><strong>SUMMARY</strong></u></p>\n",
      "<ul>\n",
      "<li>8+ years of experience in IT and Teradata administration, designing database solutions with Architecture.</li>\n",
      "<li>5+ Years of exclusive Production support and deploy of Teradata Patches, fixtures and DBQL settings.</li>\n",
      "<li>Experienced in archiving, restoring and recovering data on Teradata using ARC utility and TARA GUI.</li>\n",
      "<li>Excellent knowledge in moving database objects between Teradata systems and Teradata Database.</li>\n",
      "<li>Very well trained and experienced in scheduling backups and recovery of the entire EDW databases across various geographical locations for the business continuity and response time.</li>\n",
      "<li>Track record in Archiving and Recovering Down Amps</li>\n",
      "<li>Highly experienced in designing and testing Node fail over tests</li>\n",
      "<li>Highly knowledgeable in designing table DDL mechanisms for Automatic Data Protection</li>\n",
      "<li>Skilled in using Utility commands (TPA resets) for stopping and restarting Teradata Database.</li>\n",
      "<li>Efficient in housekeeping and maintaining database through regular cleanup of old logs, deleting old data, collecting statistics and tuning queries for efficient database running.</li>\n",
      "<li>Highly skilled in using tools for Managing Teradata Resources to minimize the occurrence of impeded performance, maximize throughput and manage of consumption of resources.</li>\n",
      "<li>Experienced in troubleshooting and analyzing problems like job hangs, slowdowns, inconsistent rows, revalidating headers for tables with RI constraints, PPIs and configuration</li>\n",
      "<li>Worked with Teradata field engineers in handling Teradata crash dumps.</li>\n",
      "<li>Handling efficient way of data and DDL backups and environment refreshes.</li>\n",
      "<li>Experienced in reading DBQL, PMON to identify monster queries and Hot Amps.</li>\n",
      "<li>Excellent planner for releases and Outage notifications.</li>\n",
      "<li>High expertise in cross database coding/design skills( Oracle, Teradata and SQLServer)</li>\n",
      "<li>Excellent track record in tuning ETL and SQL Jobs within the batch window</li>\n",
      "<li>Expert knowledge in DBS control and settings.</li>\n",
      "<li>Good coding and tuning skills using Teradata load utilities like Fastload and Teradata Parallel Transporter.</li>\n",
      "<li>Expert in UNIX scripts for Database extraction and filtering.</li>\n",
      "<li>Experienced using crontab utilities and shell scripting for scheduling weekly and nightly loads.</li>\n",
      "<li>Good hands on experience in coding and developing Autosys scripts.</li>\n",
      "<li>Designed the Utility testing environment for concurrent Loads in the Database (Teradata) box.</li>\n",
      "<li>Developed and deployed solutions that support integration of data in all environments - transactional as well as operational and analytical.</li>\n",
      "<li>Provided a solid basis for expediting transactions, streamlining operations, making optimal decisions, building large data warehouses, data marts.</li>\n",
      "<li>Implemented, Architected and Designed ETL story boards based on Hybrid model(Star and Snowflake)</li>\n",
      "<li>Experienced with supply chain management systems development, implementation and analysis.</li>\n",
      "<li>Excellent knowledge in Entity Relationship modeling and Physical database designing.</li>\n",
      "<li>Wrote SQL scripts for backend databases, custom stored procedures, macros. Enhanced and Customized load and back up scripts to the newly developed and in design projects.</li>\n",
      "</ul>\n",
      "<p><u><strong>TECHNICAL SKILLS</strong></u></p>\n",
      "<p><strong>Operating Systems: </strong>Windows 95/98, Windows 2000, XP, UNIX, MS-DOS.</p>\n",
      "<p><strong>Programming Languages: </strong>C, C++, Universe Basic, PERL, TCL, UNIX shell scripts.</p>\n",
      "<p><strong>Tools:</strong> ERWIN Data Modeling, Rational Tools, Golden Gate</p>\n",
      "<p><strong>Databases: </strong>Teradata 13.10/13.0.1// V12/6.2, Oracle 10g/9i, Universe DB, MS Access, SQL Server.</p>\n",
      "<p><u><strong>PROFESSIONAL EXPERIENCE</strong></u></p>\n",
      "<p><strong>Confidential, Hoffman Estates, IL</strong></p>\n",
      "<p><strong>Teradata DBA</strong></p>\n",
      "<p><strong>Responsibilities:</strong></p>\n",
      "<ul>\n",
      "<li>Responsible and involved in Setting up the Teradata database, users, Roles and Profiles.</li>\n",
      "<li>Applied data protection including Transient Journal, Fallback. Resolved deadlocks and DBQL maintenance creating an archive DBQL SYSTEM MGMT database.</li>\n",
      "<li>Defined Permanent space limits both at the database and user level for different business scenarios.</li>\n",
      "<li>Configured DBS control settings and MaxLoad AWT, MaxLoadTasks, DBQLFlushRate and system fields and other Performance fields like MaxParseTreeSegs.</li>\n",
      "<li>Took crash dumps and TSETs for the Teradata technology to analyze for certain crashes.</li>\n",
      "<li>Highly experienced in working with Teradata support team for Opening and tracking incidents for Teradata issues or any upgrades.</li>\n",
      "<li>Designed Security model based on the business model and enforced the security through roles.</li>\n",
      "<li>Worked with Teradata System Engineers and Support team to fix Node level issue.</li>\n",
      "<li>Installed patches to BAR server and Viewpoint.</li>\n",
      "<li>Developed statistics macros and automated to run based on the frequency.</li>\n",
      "<li>Used Dataflux tool for data profiling, data patterns and analyzing data for designing right Primary Indexes.</li>\n",
      "<li>Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.</li>\n",
      "<li>Configured the TDWM and customized for the Searsâ€™s business model.</li>\n",
      "<li>Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.</li>\n",
      "<li>Controlled and tracked access to Teradata Database by granting and revoking privileges.</li>\n",
      "<li>Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment</li>\n",
      "<li>Involved in SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades</li>\n",
      "<li>Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution</li>\n",
      "<li>Assisted Developers with coding and effective Join issues</li>\n",
      "<li>Responsible for Backups every night after Major loads.</li>\n",
      "<li>Allocated spaces to the users, controlled Spool spaces and assigning of table spaces</li>\n",
      "<li>Planned the releases, monitored performance and reported to Teradata for further technical issues.</li>\n",
      "<li>Explained Users by showing Viewpoint about their inefficient queries</li>\n",
      "<li>Involved with Teradata to deploy patches, install, fix and figured out the settings.</li>\n",
      "<li>Applied DBQL settings to the business and application standards.</li>\n",
      "</ul>\n",
      "<p><strong>Environment:</strong> Teradata 13.10, Teradata Administrator, Teradata SQL Assistant, Teradata Viewpoint, BTEQ, MLOAD, TPT, ARCHMAIN, TASM, Netbackup, TARA GUI, UNIX, Shell scripts.</p>\n",
      "<p><strong>Confidential, Riverwoods, IL</strong></p>\n",
      "<p><strong>Teradata DBA</strong></p>\n",
      "<p><strong>Responsibilities:</strong></p>\n",
      "<ul>\n",
      "<li>Involved in Setting up the Teradata database, Created databases and users, Allocated perm and spool space</li>\n",
      "<li>Applied data protection including Transient Journal, Fallback. Resolved deadlocks and involved in Granting and revoke security privileges.</li>\n",
      "<li>Defined Permanent space limits both at the database and user level.</li>\n",
      "<li>Created users, databases, roles, profiles and accounts.</li>\n",
      "<li>Established logon security, including external authentication.</li>\n",
      "<li>Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.</li>\n",
      "<li>Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.</li>\n",
      "<li>Controlled and tracked access to Teradata Database by granting and revoking privileges.</li>\n",
      "<li>Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades</li>\n",
      "<li>Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution</li>\n",
      "<li>Assisted Developers with coding and effective Join issues</li>\n",
      "<li>Responsible for Backups every night after Major loads.</li>\n",
      "<li>Allocated spaces to the users, controlled Spool spaces and assigning of table spaces</li>\n",
      "<li>Planned the releases, monitored performance and reported to Teradata for further technical issues.</li>\n",
      "<li>Involved with Teradata to deploy patches, install, fix and figured out the settings.</li>\n",
      "<li>Applied DBQL settings to the business and application standards.</li>\n",
      "<li>Hand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL Developers in Understanding Data Models, identifying the various relationships among the tables in the Model. Written the best of Programming Logic to serve the BEST purpose.</li>\n",
      "<li>Developed Data Extractions which includes Unit and System Testing of all the Extractions. Involved in Reporting the Data and its Quality Issues to Profile team.</li>\n",
      "<li>Developed SQL Scripts in Teradata, Tuned Tpump jobs to avoid deadlocks.</li>\n",
      "<li>Ran DBQL and Explains to see the in depth frame of the query behavior</li>\n",
      "<li>Developed trend charts to monitor defect levels in databases in order to maintain statistical control of the business and site systems</li>\n",
      "<li>Developed Quality Stage macros to run in batch during specific times of the day.</li>\n",
      "<li>Expert in employing Total Quality Management with data modeling and RDBMS concepts in managing data quality and clean up efforts.</li>\n",
      "<li>Implemented triggers to monitor restricted tables and database access</li>\n",
      "<li>Involved in ETL Architecture from Source data flow to Target loading</li>\n",
      "<li>Designed high volume tables with efficient Primary Indexes and Primary Partition Indexes.</li>\n",
      "<li>Designed ICD system which is a common interface between teams for data communication</li>\n",
      "<li>Acted as the primary focal point of Contact for the IBI team for both business and technical support.</li>\n",
      "<li>Coded UNIX scripts for Database extraction and filtering.</li>\n",
      "<li>Responsible for Schema comparison and DDL validation.</li>\n",
      "<li>Designed ETL version of data model for the developers to understand the modeling from the developed logical and physical models.</li>\n",
      "</ul>\n",
      "<p><strong>Environment:</strong> Teradata V2R6/12, TASM, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, UNIX, LINUX SUSE, Shell scripts.</p>\n",
      "<p><strong>Confidential, Seattle, WA</strong></p>\n",
      "<p><strong>Teradata Application DBA</strong></p>\n",
      "<p><strong>Responsibilities:</strong></p>\n",
      "<ul>\n",
      "<li>Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.</li>\n",
      "<li>Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.</li>\n",
      "<li>Controlled and tracked access to Teradata Database by granting and revoking privileges.</li>\n",
      "<li>Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades</li>\n",
      "<li>Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution</li>\n",
      "<li>Assisted Developers with coding and effective Join issues</li>\n",
      "<li>Responsible for Backups every night after Major loads.</li>\n",
      "<li>Allocated spaces to the users, controlled Spool spaces and assigning of table spaces</li>\n",
      "<li>Planned the releases, monitored performance and reported to Teradata for further technical issues.</li>\n",
      "<li>Involved with Teradata to deploy patches, install, fix and figured out the settings.</li>\n",
      "<li>Applied DBQL settings to the business and application standards.</li>\n",
      "<li>Defined relationships as the identifying and non-identifying relationships ensuring integrity constraints.</li>\n",
      "<li>Designed and Architected ETL Preprocess system to capture delta loads.</li>\n",
      "<li>Responsible in Troubleshooting and releasing Mloads in different phases and restart scenarios.</li>\n",
      "<li>Wrote UNIX shell scripts for initialization process, scheduling and control mechanism.</li>\n",
      "<li>Supported the platform along with patches.</li>\n",
      "<li>Wrote queries which are resource hog and fine tuned for performance</li>\n",
      "<li>Excellent team player and complied to security</li>\n",
      "<li>Ran DBQL and Explains to see the in depth frame of the query behavior</li>\n",
      "<li>Tutored developers regarding the PIs and Optimizer behavior about different queries.</li>\n",
      "<li>Developed sql scripts to identify data anomalies, complex data validation and for data cleansing purpose.</li>\n",
      "<li>Hand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL</li>\n",
      "<li>Interacted with Business Units and Analysts to cleanse the data and match the Operating Standards.</li>\n",
      "<li>Used rich experience of database querying in Performance tuning of ETL Jobs and embedded SQL queries in OCI, CLI, Mutliload, Tpump and SQL-Loader.</li>\n",
      "<li>Imported and Exported Repositories cross projects.</li>\n",
      "<li>Prepared functional and technical specifications for the preprocess system.</li>\n",
      "<li>Highly technical competent in all phases of application systems</li>\n",
      "<li>Designed the Utility testing environment for concurrent Loads on the Teradata box.</li>\n",
      "<li>Created efficient hash tables for referential Integrity and Lookup purposes for validation and referential purpose.</li>\n",
      "<li>Experienced in writing SQL scripts for populating new fields added to tables on a one-shot basis and solving the problem associated with slowly-changing dimension for one of the dimension tables.</li>\n",
      "<li>Involved in performing unit testing and integration testing the individual and extract-transform-load jobs in sequence respectively.</li>\n",
      "<li>Analyzed the Data Sources in identifying data anomalies patterns value ranges. Wrote SQL scripts for accomplishing the same.</li>\n",
      "<li>Compiled and debugging the Jobs based on the Errors.</li>\n",
      "<li>Wrote shell scripts for scheduling the ETL process.</li>\n",
      "</ul>\n",
      "<p><strong>Environment:</strong> Teradata V2R5/R6, TASM, TPT, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, Net vault, Erwin Designer, UNIX, Shell scripts.</p>\n",
      "<p><strong>Confidential, Atlanta, GA</strong></p>\n",
      "<p><strong>Teradata/ETL Developer</strong></p>\n",
      "<p><strong>Responsibilities:</strong></p>\n",
      "<ul>\n",
      "<li>Responsible for delivering project timeline associated with the developer deliverables.</li>\n",
      "<li>Designing and developing relational and dimensional models in Framework Manager.</li>\n",
      "<li>Used different Teradata utilities on UNIX and Mainframes (COBOL) environment.</li>\n",
      "<li>Responsible to gather requirements from business users and format them according to the business needs, model the requirements to match the current architecture and assign it to the team</li>\n",
      "<li>Educated the team on how to do the performance tuning on complex queries with efficiency of MULTI-TABLE concept, Join Index, PPI, Partitioning PPI, Secondary Index mechanism, backup and recovery, day-to-day request such as altering a table, capacity planning, general SQL tuning and skew factor.</li>\n",
      "<li>Expert in physical database design, logical modelling and development of pro-active processes for monitoring capacity and performance.</li>\n",
      "<li>Supported other ongoing projects: Software Upgrade project from Teradata V2R6.2 to V12.</li>\n",
      "<li>Highly proficient in writing Teradata load and export scripts like BTEQ, MLoad, FLoad and Fast Export.</li>\n",
      "<li>Responsible for creating Procedures and Jobs on Mainframes using JCL, to run various scripts like BTEQ, FLOAD, MLOAD and FEXPORT.</li>\n",
      "<li>Created COBOL copy books to check the format and data before sending the file to the clients.</li>\n",
      "<li>Used version control tool like Endeavor to elevate components to production environment.</li>\n",
      "<li>Educated the team on how to utilise the teradata visual tools like Teradata Manager, Database query language (DBQL), TDQM, Visual explain etc.,</li>\n",
      "<li>Implemented a process from Development life cycle to the production implementation including naming conventions, environment prefix etc and trained the team accordingly.</li>\n",
      "<li>Optimised high volume tables (Including collection tables) in teradata using various join index techniques, secondary indexes, join strategies and hash distribution methods.</li>\n",
      "<li>Implemented different kinds of purging techniques to free up space in certain teradata databases and to remove orphan records from collection tables.</li>\n",
      "<li>Trained off-shore team (four) and on-site team (three) on teradata architecture, usage of Bteq in UNIX environment and usage of MLoad, FLoad, Fast Export.</li>\n",
      "<li>Extensively used MLoad, FLoad and Fast Export Teradata tools both in ETL and UNIX scripts for high volume flat files.</li>\n",
      "<li>Created a Generic BTEQ script to load the data into various target tables from flat files using control table mechanism.</li>\n",
      "<li>Using PERL, created a script which generates MLoad script on the fly based on the table name supplied as a parameter. This has reduced the manual intervention in case of new table added in the database.</li>\n",
      "<li>Implemented data export from one environment to another environment using Fast Export and Fast Load. All that the scripts needs is table name, environment to export, environment to import and user credentials. This gives flexibility of importing data from production to test and to development as when the environments require fresh data.</li>\n",
      "<li>Faster incoming file process automated to loading into teradata staging using BTEQ, Fast Load and then loaded into MLoad by concatenating files on the basis of table name.</li>\n",
      "<li>The technologies used in processes are Teradata, oracle, DB2, UNIX and Mainframes.</li>\n",
      "<li>Extensively worked on teradata performance optimization and brought down the queries to seconds or minutes from spool out and never ending queries by using various teradata optimization strategies.</li>\n",
      "</ul>\n",
      "</div>]\n",
      "[]\n",
      "[<u><strong>OBJECTIVE</strong></u>, <u><strong>SUMMARY</strong></u>, <u><strong>TECHNICAL SKILLS</strong></u>, <u><strong>PROFESSIONAL EXPERIENCE</strong></u>]\n"
     ]
    }
   ],
   "source": [
    "def scrape_resume(url: str, session=None) -> dict:\n",
    "    \"\"\"Scrape details from an individual resume page, keeping DOM tags without duplication.\"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    sections = {}\n",
    "\n",
    "    target_divs = soup.find_all(\"div\", class_=[\"media-body\", \"single-post-body\"])\n",
    "    print(target_divs)\n",
    "    def get_real_keys(strong_tags):\n",
    "        possible_keys = {\"TECHNICAL SKILLS\", \"SKILLS\", \"PROFESSIONAL EXPERIENCE\", \"EXPERIENCE\", \"SUMMARY\", \"OBJECTIVE\"}\n",
    "        ret_list = []\n",
    "        for tag in strong_tags:\n",
    "            for possible_key in possible_keys:\n",
    "                if possible_key in tag:\n",
    "                    ret_list.append(tag)\n",
    "        return ret_list\n",
    "\n",
    "    for div in target_divs:\n",
    "        real_keys = div.find_all(\"u\")\n",
    "        # strong_tags = div.find_all(\"strong\")\n",
    "        # real_keys = get_real_keys(strong_tags)\n",
    "        print(real_keys)\n",
    "        for i, u_tag in enumerate(real_keys):\n",
    "            key = u_tag.get_text(strip=True).replace(\":\", \"\").upper()\n",
    "\n",
    "            content_tags = []\n",
    "            for sibling in u_tag.parent.next_siblings:\n",
    "                if isinstance(sibling, Tag):\n",
    "                    if sibling.find(\"u\"):\n",
    "                        break\n",
    "                    content_tags.append(sibling)\n",
    "\n",
    "            sections[key] = content_tags\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "res_dict = scrape_resume(\"https://www.hireitpeople.com/resume-database/78-oracle-dba-resumes/628271-teradata-dba-resume-hoffman-estates-il-2\")\n",
    "# for key in res_dict.keys():\n",
    "#     print(key, res_dict[key], end=\"\\n -------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93218cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "import time, random #noqa\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    job_role: str\n",
    "    responsibilities: List[str]\n",
    "    environment: Optional[str] = None\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    job_role: str\n",
    "    professional_summary: List[str]\n",
    "    technical_skills: List[str]\n",
    "    experiences: List[Experience]\n",
    "\n",
    "\n",
    "class PostExtractionResult(BaseModel):\n",
    "    \"\"\"Result model for post extraction step.\"\"\"\n",
    "    job_role: Optional[str]\n",
    "    structured_content: List[dict]\n",
    "    full_text: str\n",
    "    container_text: str\n",
    "    missing_excerpt: str\n",
    "    skipped_blocks: List[str]\n",
    "    warnings: List[str]\n",
    "    \n",
    "def normalize_breaks(soup):\n",
    "    \"\"\"Convert <br> tags to newline text nodes so .get_text() uses them.\"\"\"\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    lines = [ln.strip() for ln in text.splitlines()]\n",
    "    while lines and lines[0] == \"\":\n",
    "        lines.pop(0)\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return \"\\n\".join([re.sub(r'\\s+', ' ', ln) for ln in lines])\n",
    "\n",
    "def extract_job_role(soup):\n",
    "    media_bodies = soup.find_all(\"div\", class_=re.compile(r\"media-body\"))\n",
    "    if media_bodies:\n",
    "        media_body = media_bodies[0]\n",
    "        job_title_tag = media_body.find(\"h3\")\n",
    "        if job_title_tag:\n",
    "            job_role = job_title_tag.get_text(strip=True)\n",
    "            if job_role:\n",
    "                return job_role\n",
    "    return None\n",
    "\n",
    "def extract_post_body_safe(\n",
    "    url: str,\n",
    "    target_class = \"single-post-body\",\n",
    "    class_regex: Optional[str] = None,\n",
    "    allow_fallback: bool = True,\n",
    "    debug: bool = False,\n",
    "    min_word_threshold: int = 120,\n",
    "    retries: int = 3,\n",
    ") -> PostExtractionResult:\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\"}\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with requests.Session() as session:\n",
    "                session.headers.update(headers)\n",
    "                resp = session.get(url, timeout=20)\n",
    "                resp.raise_for_status()\n",
    "                if len(resp.text) < 1000:\n",
    "                    raise ValueError(\"Response too short.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    normalize_breaks(soup)\n",
    "\n",
    "    # Identify container\n",
    "    container = None\n",
    "    if target_class:\n",
    "        container = soup.find(\"div\", class_=target_class)\n",
    "    if not container and class_regex:\n",
    "        container = soup.find(\"div\", class_=re.compile(class_regex))\n",
    "    if not container and allow_fallback:\n",
    "        divs = soup.find_all(\"div\")\n",
    "        if divs:\n",
    "            container = max(divs, key=lambda d: len(d.get_text(strip=True)))\n",
    "    if not container:\n",
    "        raise ValueError(\"Could not find a suitable container.\")\n",
    "\n",
    "    raw_container_text = container.get_text(separator=\"\\n\", strip=True)\n",
    "    container_text = clean_whitespace(raw_container_text)\n",
    "\n",
    "    structured_content = []\n",
    "    skipped_blocks = []  # ðŸ”¹ Track skipped text blocks\n",
    "\n",
    "    resume_job_role = extract_job_role(soup)\n",
    "\n",
    "    # 2ï¸âƒ£ Handle normal paragraphs and lists not under media-body\n",
    "    for element in container.find_all([\"p\", \"ul\"], recursive=True):\n",
    "        if element.find_parent(\"div\", class_=re.compile(r\"media-body\")):\n",
    "            continue  # already captured above\n",
    "\n",
    "        if element.name == \"p\":\n",
    "            text = clean_whitespace(\" \".join(element.stripped_strings))\n",
    "            if len(text.split()) > min_word_threshold:\n",
    "                skipped_blocks.append(text[:120] + \"...\")\n",
    "                continue\n",
    "            if text:\n",
    "                structured_content.append({\"type\": \"p\", \"text\": text})\n",
    "\n",
    "        elif element.name == \"ul\":\n",
    "            items = []\n",
    "            for li in element.find_all(\"li\", recursive=False):\n",
    "                li_text = clean_whitespace(\" \".join(li.stripped_strings))\n",
    "                if len(li_text.split()) > min_word_threshold:\n",
    "                    skipped_blocks.append(li_text[:120] + \"...\")\n",
    "                    continue\n",
    "                if li_text:\n",
    "                    items.append(li_text)\n",
    "            if items:\n",
    "                structured_content.append({\"type\": \"ul\", \"items\": items})\n",
    "\n",
    "    # Join all paragraph text for convenience\n",
    "    joined_p = \"\\n\\n\".join(\n",
    "        [b[\"text\"] for b in structured_content if b.get(\"type\") == \"p\"]\n",
    "    )\n",
    "\n",
    "    container_words = len(container_text.split())\n",
    "    joined_words = len(joined_p.split()) if joined_p else 0\n",
    "\n",
    "    warnings = []\n",
    "    missing_excerpt = \"\"\n",
    "    if container_words > joined_words + 20:\n",
    "        temp = container_text\n",
    "        for block in structured_content:\n",
    "            if block.get(\"type\") == \"p\":\n",
    "                temp = temp.replace(block[\"text\"], \"\")\n",
    "            elif block.get(\"type\") == \"ul\":\n",
    "                for item in block[\"items\"]:\n",
    "                    temp = temp.replace(item, \"\")\n",
    "            elif \"job_role\" in block:\n",
    "                temp = temp.replace(block[\"job_role\"], \"\")\n",
    "                for sub in block.get(\"content\", []):\n",
    "                    if sub.get(\"type\") == \"p\":\n",
    "                        temp = temp.replace(sub[\"text\"], \"\")\n",
    "                    elif sub.get(\"type\") == \"ul\":\n",
    "                        for item in sub[\"items\"]:\n",
    "                            temp = temp.replace(item, \"\")\n",
    "        missing_excerpt = temp.strip()[:800]\n",
    "        if missing_excerpt:\n",
    "            warnings.append(\"Container has additional text not captured by structured tags.\")\n",
    "\n",
    "    if \"<script\" in resp.text.lower() and (container_words == 0 or joined_words == 0):\n",
    "        warnings.append(\"Page might be JS-rendered.\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"===== DEBUG INFO =====\")\n",
    "        print(\"Container classes:\", container.get(\"class\"))\n",
    "        print(\"Job roles found:\", sum(1 for b in structured_content if \"job_role\" in b))\n",
    "        print(\"Paragraphs:\", sum(1 for b in structured_content if b.get(\"type\") == \"p\"))\n",
    "        print(\"Lists:\", sum(1 for b in structured_content if b.get(\"type\") == \"ul\"))\n",
    "        print(\"Skipped blocks:\", len(skipped_blocks))\n",
    "        print(\"Warnings:\", warnings)\n",
    "        print(\"======================\")\n",
    "\n",
    "    return PostExtractionResult(\n",
    "        job_role=resume_job_role,\n",
    "        structured_content=structured_content,\n",
    "        full_text=joined_p,\n",
    "        container_text=container_text,\n",
    "        missing_excerpt=missing_excerpt,\n",
    "        skipped_blocks=skipped_blocks,\n",
    "        warnings=warnings,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5256456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = extract_post_body_safe(url='https://www.hireitpeople.com/resume-database/78-oracle-dba-resumes/628271-teradata-dba-resume-hoffman-estates-il-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fe72b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_role': 'Teradata Dba Resume',\n",
       " 'structured_content': [{'type': 'p', 'text': 'OBJECTIVE'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['Position where Database Designing and DBA skills and experience in database designing, administration and Management will add value in todayâ€™s challenging business and Information Technology.']},\n",
       "  {'type': 'p', 'text': 'SUMMARY'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['8+ years of experience in IT and Teradata administration, designing database solutions with Architecture.',\n",
       "    '5+ Years of exclusive Production support and deploy of Teradata Patches, fixtures and DBQL settings.',\n",
       "    'Experienced in archiving, restoring and recovering data on Teradata using ARC utility and TARA GUI.',\n",
       "    'Excellent knowledge in moving database objects between Teradata systems and Teradata Database.',\n",
       "    'Very well trained and experienced in scheduling backups and recovery of the entire EDW databases across various geographical locations for the business continuity and response time.',\n",
       "    'Track record in Archiving and Recovering Down Amps',\n",
       "    'Highly experienced in designing and testing Node fail over tests',\n",
       "    'Highly knowledgeable in designing table DDL mechanisms for Automatic Data Protection',\n",
       "    'Skilled in using Utility commands (TPA resets) for stopping and restarting Teradata Database.',\n",
       "    'Efficient in housekeeping and maintaining database through regular cleanup of old logs, deleting old data, collecting statistics and tuning queries for efficient database running.',\n",
       "    'Highly skilled in using tools for Managing Teradata Resources to minimize the occurrence of impeded performance, maximize throughput and manage of consumption of resources.',\n",
       "    'Experienced in troubleshooting and analyzing problems like job hangs, slowdowns, inconsistent rows, revalidating headers for tables with RI constraints, PPIs and configuration',\n",
       "    'Worked with Teradata field engineers in handling Teradata crash dumps.',\n",
       "    'Handling efficient way of data and DDL backups and environment refreshes.',\n",
       "    'Experienced in reading DBQL, PMON to identify monster queries and Hot Amps.',\n",
       "    'Excellent planner for releases and Outage notifications.',\n",
       "    'High expertise in cross database coding/design skills( Oracle, Teradata and SQLServer)',\n",
       "    'Excellent track record in tuning ETL and SQL Jobs within the batch window',\n",
       "    'Expert knowledge in DBS control and settings.',\n",
       "    'Good coding and tuning skills using Teradata load utilities like Fastload and Teradata Parallel Transporter.',\n",
       "    'Expert in UNIX scripts for Database extraction and filtering.',\n",
       "    'Experienced using crontab utilities and shell scripting for scheduling weekly and nightly loads.',\n",
       "    'Good hands on experience in coding and developing Autosys scripts.',\n",
       "    'Designed the Utility testing environment for concurrent Loads in the Database (Teradata) box.',\n",
       "    'Developed and deployed solutions that support integration of data in all environments - transactional as well as operational and analytical.',\n",
       "    'Provided a solid basis for expediting transactions, streamlining operations, making optimal decisions, building large data warehouses, data marts.',\n",
       "    'Implemented, Architected and Designed ETL story boards based on Hybrid model(Star and Snowflake)',\n",
       "    'Experienced with supply chain management systems development, implementation and analysis.',\n",
       "    'Excellent knowledge in Entity Relationship modeling and Physical database designing.',\n",
       "    'Wrote SQL scripts for backend databases, custom stored procedures, macros. Enhanced and Customized load and back up scripts to the newly developed and in design projects.']},\n",
       "  {'type': 'p', 'text': 'TECHNICAL SKILLS'},\n",
       "  {'type': 'p',\n",
       "   'text': 'Operating Systems: Windows 95/98, Windows 2000, XP, UNIX, MS-DOS.'},\n",
       "  {'type': 'p',\n",
       "   'text': 'Programming Languages: C, C++, Universe Basic, PERL, TCL, UNIX shell scripts.'},\n",
       "  {'type': 'p',\n",
       "   'text': 'Tools: ERWIN Data Modeling, Rational Tools, Golden Gate'},\n",
       "  {'type': 'p',\n",
       "   'text': 'Databases: Teradata 13.10/13.0.1// V12/6.2, Oracle 10g/9i, Universe DB, MS Access, SQL Server.'},\n",
       "  {'type': 'p', 'text': 'PROFESSIONAL EXPERIENCE'},\n",
       "  {'type': 'p', 'text': 'Confidential, Hoffman Estates, IL'},\n",
       "  {'type': 'p', 'text': 'Teradata DBA'},\n",
       "  {'type': 'p', 'text': 'Responsibilities:'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['Responsible and involved in Setting up the Teradata database, users, Roles and Profiles.',\n",
       "    'Applied data protection including Transient Journal, Fallback. Resolved deadlocks and DBQL maintenance creating an archive DBQL SYSTEM MGMT database.',\n",
       "    'Defined Permanent space limits both at the database and user level for different business scenarios.',\n",
       "    'Configured DBS control settings and MaxLoad AWT, MaxLoadTasks, DBQLFlushRate and system fields and other Performance fields like MaxParseTreeSegs.',\n",
       "    'Took crash dumps and TSETs for the Teradata technology to analyze for certain crashes.',\n",
       "    'Highly experienced in working with Teradata support team for Opening and tracking incidents for Teradata issues or any upgrades.',\n",
       "    'Designed Security model based on the business model and enforced the security through roles.',\n",
       "    'Worked with Teradata System Engineers and Support team to fix Node level issue.',\n",
       "    'Installed patches to BAR server and Viewpoint.',\n",
       "    'Developed statistics macros and automated to run based on the frequency.',\n",
       "    'Used Dataflux tool for data profiling, data patterns and analyzing data for designing right Primary Indexes.',\n",
       "    'Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.',\n",
       "    'Configured the TDWM and customized for the Searsâ€™s business model.',\n",
       "    'Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.',\n",
       "    'Controlled and tracked access to Teradata Database by granting and revoking privileges.',\n",
       "    'Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment',\n",
       "    'Involved in SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades',\n",
       "    'Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution',\n",
       "    'Assisted Developers with coding and effective Join issues',\n",
       "    'Responsible for Backups every night after Major loads.',\n",
       "    'Allocated spaces to the users, controlled Spool spaces and assigning of table spaces',\n",
       "    'Planned the releases, monitored performance and reported to Teradata for further technical issues.',\n",
       "    'Explained Users by showing Viewpoint about their inefficient queries',\n",
       "    'Involved with Teradata to deploy patches, install, fix and figured out the settings.',\n",
       "    'Applied DBQL settings to the business and application standards.']},\n",
       "  {'type': 'p',\n",
       "   'text': 'Environment: Teradata 13.10, Teradata Administrator, Teradata SQL Assistant, Teradata Viewpoint, BTEQ, MLOAD, TPT, ARCHMAIN, TASM, Netbackup, TARA GUI, UNIX, Shell scripts.'},\n",
       "  {'type': 'p', 'text': 'Confidential, Riverwoods, IL'},\n",
       "  {'type': 'p', 'text': 'Teradata DBA'},\n",
       "  {'type': 'p', 'text': 'Responsibilities:'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['Involved in Setting up the Teradata database, Created databases and users, Allocated perm and spool space',\n",
       "    'Applied data protection including Transient Journal, Fallback. Resolved deadlocks and involved in Granting and revoke security privileges.',\n",
       "    'Defined Permanent space limits both at the database and user level.',\n",
       "    'Created users, databases, roles, profiles and accounts.',\n",
       "    'Established logon security, including external authentication.',\n",
       "    'Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.',\n",
       "    'Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.',\n",
       "    'Controlled and tracked access to Teradata Database by granting and revoking privileges.',\n",
       "    'Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades',\n",
       "    'Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution',\n",
       "    'Assisted Developers with coding and effective Join issues',\n",
       "    'Responsible for Backups every night after Major loads.',\n",
       "    'Allocated spaces to the users, controlled Spool spaces and assigning of table spaces',\n",
       "    'Planned the releases, monitored performance and reported to Teradata for further technical issues.',\n",
       "    'Involved with Teradata to deploy patches, install, fix and figured out the settings.',\n",
       "    'Applied DBQL settings to the business and application standards.',\n",
       "    'Hand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL Developers in Understanding Data Models, identifying the various relationships among the tables in the Model. Written the best of Programming Logic to serve the BEST purpose.',\n",
       "    'Developed Data Extractions which includes Unit and System Testing of all the Extractions. Involved in Reporting the Data and its Quality Issues to Profile team.',\n",
       "    'Developed SQL Scripts in Teradata, Tuned Tpump jobs to avoid deadlocks.',\n",
       "    'Ran DBQL and Explains to see the in depth frame of the query behavior',\n",
       "    'Developed trend charts to monitor defect levels in databases in order to maintain statistical control of the business and site systems',\n",
       "    'Developed Quality Stage macros to run in batch during specific times of the day.',\n",
       "    'Expert in employing Total Quality Management with data modeling and RDBMS concepts in managing data quality and clean up efforts.',\n",
       "    'Implemented triggers to monitor restricted tables and database access',\n",
       "    'Involved in ETL Architecture from Source data flow to Target loading',\n",
       "    'Designed high volume tables with efficient Primary Indexes and Primary Partition Indexes.',\n",
       "    'Designed ICD system which is a common interface between teams for data communication',\n",
       "    'Acted as the primary focal point of Contact for the IBI team for both business and technical support.',\n",
       "    'Coded UNIX scripts for Database extraction and filtering.',\n",
       "    'Responsible for Schema comparison and DDL validation.',\n",
       "    'Designed ETL version of data model for the developers to understand the modeling from the developed logical and physical models.']},\n",
       "  {'type': 'p',\n",
       "   'text': 'Environment: Teradata V2R6/12, TASM, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, UNIX, LINUX SUSE, Shell scripts.'},\n",
       "  {'type': 'p', 'text': 'Confidential, Seattle, WA'},\n",
       "  {'type': 'p', 'text': 'Teradata Application DBA'},\n",
       "  {'type': 'p', 'text': 'Responsibilities:'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['Defined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.',\n",
       "    'Experienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.',\n",
       "    'Controlled and tracked access to Teradata Database by granting and revoking privileges.',\n",
       "    'Implemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades',\n",
       "    'Designed DDLs and efficient PIs along with Identity Keys for efficient data distribution',\n",
       "    'Assisted Developers with coding and effective Join issues',\n",
       "    'Responsible for Backups every night after Major loads.',\n",
       "    'Allocated spaces to the users, controlled Spool spaces and assigning of table spaces',\n",
       "    'Planned the releases, monitored performance and reported to Teradata for further technical issues.',\n",
       "    'Involved with Teradata to deploy patches, install, fix and figured out the settings.',\n",
       "    'Applied DBQL settings to the business and application standards.',\n",
       "    'Defined relationships as the identifying and non-identifying relationships ensuring integrity constraints.',\n",
       "    'Designed and Architected ETL Preprocess system to capture delta loads.',\n",
       "    'Responsible in Troubleshooting and releasing Mloads in different phases and restart scenarios.',\n",
       "    'Wrote UNIX shell scripts for initialization process, scheduling and control mechanism.',\n",
       "    'Supported the platform along with patches.',\n",
       "    'Wrote queries which are resource hog and fine tuned for performance',\n",
       "    'Excellent team player and complied to security',\n",
       "    'Ran DBQL and Explains to see the in depth frame of the query behavior',\n",
       "    'Tutored developers regarding the PIs and Optimizer behavior about different queries.',\n",
       "    'Developed sql scripts to identify data anomalies, complex data validation and for data cleansing purpose.',\n",
       "    'Hand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL',\n",
       "    'Interacted with Business Units and Analysts to cleanse the data and match the Operating Standards.',\n",
       "    'Used rich experience of database querying in Performance tuning of ETL Jobs and embedded SQL queries in OCI, CLI, Mutliload, Tpump and SQL-Loader.',\n",
       "    'Imported and Exported Repositories cross projects.',\n",
       "    'Prepared functional and technical specifications for the preprocess system.',\n",
       "    'Highly technical competent in all phases of application systems',\n",
       "    'Designed the Utility testing environment for concurrent Loads on the Teradata box.',\n",
       "    'Created efficient hash tables for referential Integrity and Lookup purposes for validation and referential purpose.',\n",
       "    'Experienced in writing SQL scripts for populating new fields added to tables on a one-shot basis and solving the problem associated with slowly-changing dimension for one of the dimension tables.',\n",
       "    'Involved in performing unit testing and integration testing the individual and extract-transform-load jobs in sequence respectively.',\n",
       "    'Analyzed the Data Sources in identifying data anomalies patterns value ranges. Wrote SQL scripts for accomplishing the same.',\n",
       "    'Compiled and debugging the Jobs based on the Errors.',\n",
       "    'Wrote shell scripts for scheduling the ETL process.']},\n",
       "  {'type': 'p',\n",
       "   'text': 'Environment: Teradata V2R5/R6, TASM, TPT, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, Net vault, Erwin Designer, UNIX, Shell scripts.'},\n",
       "  {'type': 'p', 'text': 'Confidential, Atlanta, GA'},\n",
       "  {'type': 'p', 'text': 'Teradata/ETL Developer'},\n",
       "  {'type': 'p', 'text': 'Responsibilities:'},\n",
       "  {'type': 'ul',\n",
       "   'items': ['Responsible for delivering project timeline associated with the developer deliverables.',\n",
       "    'Designing and developing relational and dimensional models in Framework Manager.',\n",
       "    'Used different Teradata utilities on UNIX and Mainframes (COBOL) environment.',\n",
       "    'Responsible to gather requirements from business users and format them according to the business needs, model the requirements to match the current architecture and assign it to the team',\n",
       "    'Educated the team on how to do the performance tuning on complex queries with efficiency of MULTI-TABLE concept, Join Index, PPI, Partitioning PPI, Secondary Index mechanism, backup and recovery, day-to-day request such as altering a table, capacity planning, general SQL tuning and skew factor.',\n",
       "    'Expert in physical database design, logical modelling and development of pro-active processes for monitoring capacity and performance.',\n",
       "    'Supported other ongoing projects: Software Upgrade project from Teradata V2R6.2 to V12.',\n",
       "    'Highly proficient in writing Teradata load and export scripts like BTEQ, MLoad, FLoad and Fast Export.',\n",
       "    'Responsible for creating Procedures and Jobs on Mainframes using JCL, to run various scripts like BTEQ, FLOAD, MLOAD and FEXPORT.',\n",
       "    'Created COBOL copy books to check the format and data before sending the file to the clients.',\n",
       "    'Used version control tool like Endeavor to elevate components to production environment.',\n",
       "    'Educated the team on how to utilise the teradata visual tools like Teradata Manager, Database query language (DBQL), TDQM, Visual explain etc.,',\n",
       "    'Implemented a process from Development life cycle to the production implementation including naming conventions, environment prefix etc and trained the team accordingly.',\n",
       "    'Optimised high volume tables (Including collection tables) in teradata using various join index techniques, secondary indexes, join strategies and hash distribution methods.',\n",
       "    'Implemented different kinds of purging techniques to free up space in certain teradata databases and to remove orphan records from collection tables.',\n",
       "    'Trained off-shore team (four) and on-site team (three) on teradata architecture, usage of Bteq in UNIX environment and usage of MLoad, FLoad, Fast Export.',\n",
       "    'Extensively used MLoad, FLoad and Fast Export Teradata tools both in ETL and UNIX scripts for high volume flat files.',\n",
       "    'Created a Generic BTEQ script to load the data into various target tables from flat files using control table mechanism.',\n",
       "    'Using PERL, created a script which generates MLoad script on the fly based on the table name supplied as a parameter. This has reduced the manual intervention in case of new table added in the database.',\n",
       "    'Implemented data export from one environment to another environment using Fast Export and Fast Load. All that the scripts needs is table name, environment to export, environment to import and user credentials. This gives flexibility of importing data from production to test and to development as when the environments require fresh data.',\n",
       "    'Faster incoming file process automated to loading into teradata staging using BTEQ, Fast Load and then loaded into MLoad by concatenating files on the basis of table name.',\n",
       "    'The technologies used in processes are Teradata, oracle, DB2, UNIX and Mainframes.',\n",
       "    'Extensively worked on teradata performance optimization and brought down the queries to seconds or minutes from spool out and never ending queries by using various teradata optimization strategies.']}],\n",
       " 'full_text': 'OBJECTIVE\\n\\nSUMMARY\\n\\nTECHNICAL SKILLS\\n\\nOperating Systems: Windows 95/98, Windows 2000, XP, UNIX, MS-DOS.\\n\\nProgramming Languages: C, C++, Universe Basic, PERL, TCL, UNIX shell scripts.\\n\\nTools: ERWIN Data Modeling, Rational Tools, Golden Gate\\n\\nDatabases: Teradata 13.10/13.0.1// V12/6.2, Oracle 10g/9i, Universe DB, MS Access, SQL Server.\\n\\nPROFESSIONAL EXPERIENCE\\n\\nConfidential, Hoffman Estates, IL\\n\\nTeradata DBA\\n\\nResponsibilities:\\n\\nEnvironment: Teradata 13.10, Teradata Administrator, Teradata SQL Assistant, Teradata Viewpoint, BTEQ, MLOAD, TPT, ARCHMAIN, TASM, Netbackup, TARA GUI, UNIX, Shell scripts.\\n\\nConfidential, Riverwoods, IL\\n\\nTeradata DBA\\n\\nResponsibilities:\\n\\nEnvironment: Teradata V2R6/12, TASM, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, UNIX, LINUX SUSE, Shell scripts.\\n\\nConfidential, Seattle, WA\\n\\nTeradata Application DBA\\n\\nResponsibilities:\\n\\nEnvironment: Teradata V2R5/R6, TASM, TPT, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, Net vault, Erwin Designer, UNIX, Shell scripts.\\n\\nConfidential, Atlanta, GA\\n\\nTeradata/ETL Developer\\n\\nResponsibilities:',\n",
       " 'container_text': 'OBJECTIVE\\nPosition where Database Designing and DBA skills and experience in database designing, administration and Management will add value in todayâ€™s challenging business and Information Technology.\\nSUMMARY\\n8+ years of experience in IT and Teradata administration, designing database solutions with Architecture.\\n5+ Years of exclusive Production support and deploy of Teradata Patches, fixtures and DBQL settings.\\nExperienced in archiving, restoring and recovering data on Teradata using ARC utility and TARA GUI.\\nExcellent knowledge in moving database objects between Teradata systems and Teradata Database.\\nVery well trained and experienced in scheduling backups and recovery of the entire EDW databases across various geographical locations for the business continuity and response time.\\nTrack record in Archiving and Recovering Down Amps\\nHighly experienced in designing and testing Node fail over tests\\nHighly knowledgeable in designing table DDL mechanisms for Automatic Data Protection\\nSkilled in using Utility commands (TPA resets) for stopping and restarting Teradata Database.\\nEfficient in housekeeping and maintaining database through regular cleanup of old logs, deleting old data, collecting statistics and tuning queries for efficient database running.\\nHighly skilled in using tools for Managing Teradata Resources to minimize the occurrence of impeded performance, maximize throughput and manage of consumption of resources.\\nExperienced in troubleshooting and analyzing problems like job hangs, slowdowns, inconsistent rows, revalidating headers for tables with RI constraints, PPIs and configuration\\nWorked with Teradata field engineers in handling Teradata crash dumps.\\nHandling efficient way of data and DDL backups and environment refreshes.\\nExperienced in reading DBQL, PMON to identify monster queries and Hot Amps.\\nExcellent planner for releases and Outage notifications.\\nHigh expertise in cross database coding/design skills( Oracle, Teradata and SQLServer)\\nExcellent track record in tuning ETL and SQL Jobs within the batch window\\nExpert knowledge in DBS control and settings.\\nGood coding and tuning skills using Teradata load utilities like Fastload and Teradata Parallel Transporter.\\nExpert in UNIX scripts for Database extraction and filtering.\\nExperienced using crontab utilities and shell scripting for scheduling weekly and nightly loads.\\nGood hands on experience in coding and developing Autosys scripts.\\nDesigned the Utility testing environment for concurrent Loads in the Database (Teradata) box.\\nDeveloped and deployed solutions that support integration of data in all environments - transactional as well as operational and analytical.\\nProvided a solid basis for expediting transactions, streamlining operations, making optimal decisions, building large data warehouses, data marts.\\nImplemented, Architected and Designed ETL story boards based on Hybrid model(Star and Snowflake)\\nExperienced with supply chain management systems development, implementation and analysis.\\nExcellent knowledge in Entity Relationship modeling and Physical database designing.\\nWrote SQL scripts for backend databases, custom stored procedures, macros. Enhanced and Customized load and back up scripts to the newly developed and in design projects.\\nTECHNICAL SKILLS\\nOperating Systems:\\nWindows 95/98, Windows 2000, XP, UNIX, MS-DOS.\\nProgramming Languages:\\nC, C++, Universe Basic, PERL, TCL, UNIX shell scripts.\\nTools:\\nERWIN Data Modeling, Rational Tools, Golden Gate\\nDatabases:\\nTeradata 13.10/13.0.1// V12/6.2, Oracle 10g/9i, Universe DB, MS Access, SQL Server.\\nPROFESSIONAL EXPERIENCE\\nConfidential, Hoffman Estates, IL\\nTeradata DBA\\nResponsibilities:\\nResponsible and involved in Setting up the Teradata database, users, Roles and Profiles.\\nApplied data protection including Transient Journal, Fallback. Resolved deadlocks and DBQL maintenance creating an archive DBQL SYSTEM MGMT database.\\nDefined Permanent space limits both at the database and user level for different business scenarios.\\nConfigured DBS control settings and MaxLoad AWT, MaxLoadTasks, DBQLFlushRate and system fields and other Performance fields like MaxParseTreeSegs.\\nTook crash dumps and TSETs for the Teradata technology to analyze for certain crashes.\\nHighly experienced in working with Teradata support team for Opening and tracking incidents for Teradata issues or any upgrades.\\nDesigned Security model based on the business model and enforced the security through roles.\\nWorked with Teradata System Engineers and Support team to fix Node level issue.\\nInstalled patches to BAR server and Viewpoint.\\nDeveloped statistics macros and automated to run based on the frequency.\\nUsed Dataflux tool for data profiling, data patterns and analyzing data for designing right Primary Indexes.\\nDefined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.\\nConfigured the TDWM and customized for the Searsâ€™s business model.\\nExperienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.\\nControlled and tracked access to Teradata Database by granting and revoking privileges.\\nImplemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment\\nInvolved in SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades\\nDesigned DDLs and efficient PIs along with Identity Keys for efficient data distribution\\nAssisted Developers with coding and effective Join issues\\nResponsible for Backups every night after Major loads.\\nAllocated spaces to the users, controlled Spool spaces and assigning of table spaces\\nPlanned the releases, monitored performance and reported to Teradata for further technical issues.\\nExplained Users by showing Viewpoint about their inefficient queries\\nInvolved with Teradata to deploy patches, install, fix and figured out the settings.\\nApplied DBQL settings to the business and application standards.\\nEnvironment:\\nTeradata 13.10, Teradata Administrator, Teradata SQL Assistant, Teradata Viewpoint, BTEQ, MLOAD, TPT, ARCHMAIN, TASM, Netbackup, TARA GUI, UNIX, Shell scripts.\\nConfidential, Riverwoods, IL\\nTeradata DBA\\nResponsibilities:\\nInvolved in Setting up the Teradata database, Created databases and users, Allocated perm and spool space\\nApplied data protection including Transient Journal, Fallback. Resolved deadlocks and involved in Granting and revoke security privileges.\\nDefined Permanent space limits both at the database and user level.\\nCreated users, databases, roles, profiles and accounts.\\nEstablished logon security, including external authentication.\\nDefined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.\\nExperienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.\\nControlled and tracked access to Teradata Database by granting and revoking privileges.\\nImplemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades\\nDesigned DDLs and efficient PIs along with Identity Keys for efficient data distribution\\nAssisted Developers with coding and effective Join issues\\nResponsible for Backups every night after Major loads.\\nAllocated spaces to the users, controlled Spool spaces and assigning of table spaces\\nPlanned the releases, monitored performance and reported to Teradata for further technical issues.\\nInvolved with Teradata to deploy patches, install, fix and figured out the settings.\\nApplied DBQL settings to the business and application standards.\\nHand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL Developers in Understanding Data Models, identifying the various relationships among the tables in the Model. Written the best of Programming Logic to serve the BEST purpose.\\nDeveloped Data Extractions which includes Unit and System Testing of all the Extractions. Involved in Reporting the Data and its Quality Issues to Profile team.\\nDeveloped SQL Scripts in Teradata, Tuned Tpump jobs to avoid deadlocks.\\nRan DBQL and Explains to see the in depth frame of the query behavior\\nDeveloped trend charts to monitor defect levels in databases in order to maintain statistical control of the business and site systems\\nDeveloped Quality Stage macros to run in batch during specific times of the day.\\nExpert in employing Total Quality Management with data modeling and RDBMS concepts in managing data quality and clean up efforts.\\nImplemented triggers to monitor restricted tables and database access\\nInvolved in ETL Architecture from Source data flow to Target loading\\nDesigned high volume tables with efficient Primary Indexes and Primary Partition Indexes.\\nDesigned ICD system which is a common interface between teams for data communication\\nActed as the primary focal point of Contact for the IBI team for both business and technical support.\\nCoded UNIX scripts for Database extraction and filtering.\\nResponsible for Schema comparison and DDL validation.\\nDesigned ETL version of data model for the developers to understand the modeling from the developed logical and physical models.\\nEnvironment:\\nTeradata V2R6/12, TASM, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, UNIX, LINUX SUSE, Shell scripts.\\nConfidential, Seattle, WA\\nTeradata Application DBA\\nResponsibilities:\\nDefined account IDs, priority scheduler performance groups, and system date and time substitution variables in user and profile definitions.\\nExperienced in Loading, archiving and restoring data Duties also involved are storage optimization, performance tuning, monitoring, UNIX shell scripting, and physical and logical database design.\\nControlled and tracked access to Teradata Database by granting and revoking privileges.\\nImplemented Teradata protection features Table design and index selection Table implementations, maintenance, and backup, Problem support, Workload monitoring and control, Policies, procedures and guidelines that govern the Teradata environment, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades\\nDesigned DDLs and efficient PIs along with Identity Keys for efficient data distribution\\nAssisted Developers with coding and effective Join issues\\nResponsible for Backups every night after Major loads.\\nAllocated spaces to the users, controlled Spool spaces and assigning of table spaces\\nPlanned the releases, monitored performance and reported to Teradata for further technical issues.\\nInvolved with Teradata to deploy patches, install, fix and figured out the settings.\\nApplied DBQL settings to the business and application standards.\\nDefined relationships as the identifying and non-identifying relationships ensuring integrity constraints.\\nDesigned and Architected ETL Preprocess system to capture delta loads.\\nResponsible in Troubleshooting and releasing Mloads in different phases and restart scenarios.\\nWrote UNIX shell scripts for initialization process, scheduling and control mechanism.\\nSupported the platform along with patches.\\nWrote queries which are resource hog and fine tuned for performance\\nExcellent team player and complied to security\\nRan DBQL and Explains to see the in depth frame of the query behavior\\nTutored developers regarding the PIs and Optimizer behavior about different queries.\\nDeveloped sql scripts to identify data anomalies, complex data validation and for data cleansing purpose.\\nHand coded Teradata SQL queries in Teradata CLI stage, Written Macros, Stored Procedures, triggers in Teradata. Extracted valid data to avoid overhead and Designed Test cases and Error codes and involved in testing the data stage Jobs before running in Pre-Prod, also helped ETL\\nInteracted with Business Units and Analysts to cleanse the data and match the Operating Standards.\\nUsed rich experience of database querying in Performance tuning of ETL Jobs and embedded SQL queries in OCI, CLI, Mutliload, Tpump and SQL-Loader.\\nImported and Exported Repositories cross projects.\\nPrepared functional and technical specifications for the preprocess system.\\nHighly technical competent in all phases of application systems\\nDesigned the Utility testing environment for concurrent Loads on the Teradata box.\\nCreated efficient hash tables for referential Integrity and Lookup purposes for validation and referential purpose.\\nExperienced in writing SQL scripts for populating new fields added to tables on a one-shot basis and solving the problem associated with slowly-changing dimension for one of the dimension tables.\\nInvolved in performing unit testing and integration testing the individual and extract-transform-load jobs in sequence respectively.\\nAnalyzed the Data Sources in identifying data anomalies patterns value ranges. Wrote SQL scripts for accomplishing the same.\\nCompiled and debugging the Jobs based on the Errors.\\nWrote shell scripts for scheduling the ETL process.\\nEnvironment:\\nTeradata V2R5/R6, TASM, TPT, Teradata Administrator, Teradata SQL Assistant, Teradata Manager, BTEQ, PMON, MLOAD, ARCHMAIN, Net vault, Erwin Designer, UNIX, Shell scripts.\\nConfidential, Atlanta, GA\\nTeradata/ETL Developer\\nResponsibilities:\\nResponsible for delivering project timeline associated with the developer deliverables.\\nDesigning and developing relational and dimensional models in Framework Manager.\\nUsed different Teradata utilities on UNIX and Mainframes (COBOL) environment.\\nResponsible to gather requirements from business users and format them according to the business needs, model the requirements to match the current architecture and assign it to the team\\nEducated the team on how to do the performance tuning on complex queries with efficiency of MULTI-TABLE concept, Join Index, PPI, Partitioning PPI, Secondary Index mechanism, backup and recovery, day-to-day request such as altering a table, capacity planning, general SQL tuning and skew factor.\\nExpert in physical database design, logical modelling and development of pro-active processes for monitoring capacity and performance.\\nSupported other ongoing projects: Software Upgrade project from Teradata V2R6.2 to V12.\\nHighly proficient in writing Teradata load and export scripts like BTEQ, MLoad, FLoad and Fast Export.\\nResponsible for creating Procedures and Jobs on Mainframes using JCL, to run various scripts like BTEQ, FLOAD, MLOAD and FEXPORT.\\nCreated COBOL copy books to check the format and data before sending the file to the clients.\\nUsed version control tool like Endeavor to elevate components to production environment.\\nEducated the team on how to utilise the teradata visual tools like Teradata Manager, Database query language (DBQL), TDQM, Visual explain etc.,\\nImplemented a process from Development life cycle to the production implementation including naming conventions, environment prefix etc and trained the team accordingly.\\nOptimised high volume tables (Including collection tables) in teradata using various join index techniques, secondary indexes, join strategies and hash distribution methods.\\nImplemented different kinds of purging techniques to free up space in certain teradata databases and to remove orphan records from collection tables.\\nTrained off-shore team (four) and on-site team (three) on teradata architecture, usage of Bteq in UNIX environment and usage of MLoad, FLoad, Fast Export.\\nExtensively used MLoad, FLoad and Fast Export Teradata tools both in ETL and UNIX scripts for high volume flat files.\\nCreated a Generic BTEQ script to load the data into various target tables from flat files using control table mechanism.\\nUsing PERL, created a script which generates MLoad script on the fly based on the table name supplied as a parameter. This has reduced the manual intervention in case of new table added in the database.\\nImplemented data export from one environment to another environment using Fast Export and Fast Load. All that the scripts needs is table name, environment to export, environment to import and user credentials. This gives flexibility of importing data from production to test and to development as when the environments require fresh data.\\nFaster incoming file process automated to loading into teradata staging using BTEQ, Fast Load and then loaded into MLoad by concatenating files on the basis of table name.\\nThe technologies used in processes are Teradata, oracle, DB2, UNIX and Mainframes.\\nExtensively worked on teradata performance optimization and brought down the queries to seconds or minutes from spool out and never ending queries by using various teradata optimization strategies.',\n",
       " 'missing_excerpt': 'Operating Systems:\\nWindows 95/98, Windows 2000, XP, UNIX, MS-DOS.\\nProgramming Languages:\\nC, C++, Universe Basic, PERL, TCL, UNIX shell scripts.\\nTools:\\nERWIN Data Modeling, Rational Tools, Golden Gate\\nDatabases:\\nTeradata 13.10/13.0.1// V12/6.2, Oracle 10g/9i, Universe DB, MS Access, SQL Server.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironment:\\nTeradata 13.10, Teradata Administrator, Teradata SQL Assistant, Teradata Viewpoint, BTEQ, MLOAD, TPT, ARCHMAIN, TASM, Netbackup, TARA GUI, UNIX, Shell scripts.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n, SQL code review, Developer and user support and training, Capacity planning, System software testing and benchmarking and Support and coordination during hardware upgrades\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironment:\\nTeradata V2R6/12, TASM, Teradata Administrator, Teradata SQL Assistant, Teradata Ma',\n",
       " 'skipped_blocks': ['OBJECTIVE Position where Database Designing and DBA skills and experience in database designing, administration and Mana...'],\n",
       " 'warnings': ['Container has additional text not captured by structured tags.']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "541252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_professional_experience(res_dict):\n",
    "    if \"PROFESSIONAL EXPERIENCE\" not in res_dict:\n",
    "        return []\n",
    "\n",
    "    items = res_dict[\"PROFESSIONAL EXPERIENCE\"]\n",
    "    prev_col, curr_col = False, None\n",
    "    exps_list, exp_dict = [], {}\n",
    "    i = 0\n",
    "\n",
    "    while i < len(items):\n",
    "        item = items[i]\n",
    "\n",
    "        if item.find(\"strong\"):\n",
    "            strip_strong = item.get_text(strip=True)\n",
    "            curr_col = \":\" in strip_strong\n",
    "            if prev_col and not curr_col and exp_dict:\n",
    "                exps_list.append(exp_dict)\n",
    "                exp_dict = {}\n",
    "\n",
    "            exp_len = len(exp_dict)\n",
    "            if exp_len == 0:\n",
    "                exp_dict[\"company_name\"] = strip_strong\n",
    "            elif exp_len == 1:\n",
    "                exp_dict[\"job_role\"] = strip_strong\n",
    "            else:\n",
    "                text_lower = strip_strong.lower()\n",
    "\n",
    "                if \"responsi\" in text_lower:\n",
    "                    if i + 1 < len(items):\n",
    "                        exp_dict[\"responsibilities\"] = make_list(items[i+1])\n",
    "\n",
    "                elif \"environment\" in text_lower[:20]:\n",
    "                    exp_dict[\"environment\"] = \" \".join(strip_strong.split(\":\")[1:]).strip()\n",
    "\n",
    "            prev_col = curr_col\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if exp_dict:\n",
    "        exps_list.append(exp_dict)\n",
    "\n",
    "    return exps_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "914b8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<u><strong>SUMMARY</strong></u>, <u><strong>TECHNICAL SKILLS</strong></u>, <u><strong>PROFESSIONAL EXPERIENCE</strong></u>]\n",
      "3\n",
      "{'company_name': 'Confidential, New jersey',\n",
      " 'environment': 'Application Release 9.1 Tools 8.48,8.49,Microsoft Office '\n",
      "                'Tools,MS Project, Windows Server 2003, Microsoft SQL Server.',\n",
      " 'job_role': 'PeopleSoft HCM / HRMS Functional Consultant',\n",
      " 'responsibilities': ['Making of the initial project plan with activities by '\n",
      "                      'phases, key milestones and deliverables according to '\n",
      "                      'the scope of the RFP.',\n",
      "                      'Participating in meetings with members of the board '\n",
      "                      'notifying progress and proposing solutions for some '\n",
      "                      'problems within the project.',\n",
      "                      'Co-ordination with multiple teams and integration '\n",
      "                      'architects, in performing analysis for issues regarding '\n",
      "                      'speed and stability.',\n",
      "                      'Conducted JAD sessions with Management, SMEs (Subject '\n",
      "                      'Matter Expertise), Vendors, Users and other '\n",
      "                      'Stakeholders for open and pending issues to develop '\n",
      "                      'specifications',\n",
      "                      'Analyzed and prioritized user and business '\n",
      "                      'requirements, as system requirements, that were '\n",
      "                      'included while developing the software.',\n",
      "                      'Support NA Payroll, Time & Labor, Benefits '\n",
      "                      'administration.',\n",
      "                      'Setting Up Earnings, General Deductions, Benefit '\n",
      "                      'Deductions',\n",
      "                      'Setting up federal tax, state tax.',\n",
      "                      'Generating & viewing payslips using epay.',\n",
      "                      'Year end processing includes tax forms, W-2 company, '\n",
      "                      'updating retirement plan.',\n",
      "                      'Integration of payroll with benefits.',\n",
      "                      'Setting up benefit plans.',\n",
      "                      'Managing FMLA and savings plans.',\n",
      "                      'Extensively used SQL and Excel for Data Analysis.',\n",
      "                      'Prepared Use Cases, Business Process Models and Data '\n",
      "                      'flow diagrams, User Interface models.',\n",
      "                      'Extracted the Business Requirements during various '\n",
      "                      'sessions with business leads and tagged all '\n",
      "                      'requirements, tracing them all the way to the '\n",
      "                      'stakeholder requirements.',\n",
      "                      'Extensively involved in performing Gap Analysis in the '\n",
      "                      'upgrade of the existing Processing system and '\n",
      "                      'highlighting related issues.',\n",
      "                      'Responsible for weekly project dashboards, issue '\n",
      "                      'escalations and resolutions, Change Request Management '\n",
      "                      'and Defect Management.',\n",
      "                      'Extensively involved in performing the SIT (System '\n",
      "                      'Integration Testing) and UAT (User Acceptance Testing) '\n",
      "                      'testing of new system.',\n",
      "                      'Ensures the tasks is delivered on time and produces '\n",
      "                      'high quality deliverables.']}\n",
      "{'company_name': 'Confidential',\n",
      " 'environment': 'Application Release PS HRMS 8.8,Peopletools - 8.46,Microsoft '\n",
      "                'Office Tools,MS Project,Windows Server 2003, Microsoft SQL '\n",
      "                'Server.',\n",
      " 'job_role': 'PeopleSoft HCM / HRMS Functional Consultant',\n",
      " 'responsibilities': ['Gathered business requirements and formalized them '\n",
      "                      'through requirements walk through sessions.',\n",
      "                      'Developed Process Flow Diagrams, Work Flow Diagrams, '\n",
      "                      'and Use Cases using MS Visio.',\n",
      "                      'Developed Traceability Matrix, Data mapping and Data '\n",
      "                      'flow diagrams.',\n",
      "                      'Planned & implemented the Testing cycle schedules, '\n",
      "                      'timelines, test plan, Test cases & test data sets for '\n",
      "                      'system testing.',\n",
      "                      'Implementation of PeopleSoft HCM and Global Payroll '\n",
      "                      'version 9.0',\n",
      "                      'Coordinate and support the analysis, design, and '\n",
      "                      'develop phases of the project.',\n",
      "                      'Coordinated daily activities with the IT Developers, QA '\n",
      "                      'and Product teams along with the project management '\n",
      "                      'group.',\n",
      "                      'Working with payee data data retrieval from PeopleSoft '\n",
      "                      'HR, and payroll system and pay group assignments for a '\n",
      "                      'payee, adding a person .',\n",
      "                      'Processing payrolls iteratively, creating group lists '\n",
      "                      'for payroll processing.',\n",
      "                      'Integrating Absences Management with Global Payroll.',\n",
      "                      'Configuration of PeopleSoft Absence Management.',\n",
      "                      'Schedules for part time and full time employees.',\n",
      "                      'Entitlements and accruals for vacations.',\n",
      "                      'Absences types (Leave of absence, unjustified, '\n",
      "                      'vacations, PTO)',\n",
      "                      'Takes (defining rules for each absence type)',\n",
      "                      'Entry, approval (self-service functionality)',\n",
      "                      'Absence elements (element types, calculation elements)',\n",
      "                      'Effectively conducted Change Request Management and '\n",
      "                      'assigned priority on each change.',\n",
      "                      'Prepared UAT test strategy, test plans, reviewed QA '\n",
      "                      'test plans for appropriate test coverage.',\n",
      "                      'Conducted JAD Sessions with various stakeholders and '\n",
      "                      'project team.',\n",
      "                      'Managed Change Request Management and Defect '\n",
      "                      'Management.',\n",
      "                      'Conducted gap analysis of current processes and '\n",
      "                      'developed new process flow, data flow and work flow '\n",
      "                      'models.']}\n",
      "{'company_name': 'Confidential',\n",
      " 'environment': 'app version is 8.8, Tool ver- 8.46',\n",
      " 'job_role': 'PeopleSoft HCM / HRMS Functional Consultant',\n",
      " 'responsibilities': ['Coordinate and supervise the progress of all phases of '\n",
      "                      'the project, those of Strategy, Planning, Structure, '\n",
      "                      'Construct, Transition and Deploy.',\n",
      "                      'Worked with specialists (SME) in the field of '\n",
      "                      'development, users and customers, produce goal and '\n",
      "                      'target specifications and was responsible to solve any '\n",
      "                      'conflicts that arise and minimise potential risks.',\n",
      "                      'Participating in meetings with members of the board '\n",
      "                      'notifying progress and proposing solutions for some '\n",
      "                      'problems within the project.',\n",
      "                      'Organise employee groups, create schedules, workgroups.',\n",
      "                      'Report time,manage the reported time,process payable '\n",
      "                      'time.',\n",
      "                      'Approvals of reported time and payable time.',\n",
      "                      'Integration of time and labour with base '\n",
      "                      'benefits,global payroll.',\n",
      "                      'Working with payee data,data retrieval from PeopleSoft '\n",
      "                      'HR, and payroll system and pay group assignments for a '\n",
      "                      'payee, adding a person',\n",
      "                      'Processing Payroll iteratively, creating group lists '\n",
      "                      'for payroll processing.',\n",
      "                      'Viewing and Finalizing Payroll Results',\n",
      "                      'Configuring & managing Off Cycle Transactions',\n",
      "                      'Understanding How to Compensate Employees in Global '\n",
      "                      'Payroll for Time Reported Through Time and Labor',\n",
      "                      'Configuring Global Payroll to Work with Time and Labor',\n",
      "                      'Managed Change Request Management and Defect '\n",
      "                      'Management.',\n",
      "                      'Managed UAT testing and developed test strategies, test '\n",
      "                      'plans, reviewed QA test plans for appropriate test '\n",
      "                      'coverage.',\n",
      "                      'Tracked the defects in legacy reports as per end users. '\n",
      "                      'Compared the results and tracked the differences in '\n",
      "                      'excel.']}\n"
     ]
    }
   ],
   "source": [
    "res_dict = scrape_resume(test_urls[0])\n",
    "# print(res_dict)\n",
    "print(len(res_dict))\n",
    "for exp in parse_px(res_dict):\n",
    "    pprint(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
